{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('out/checkpoints/',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in data and do Preliminary processing\n",
    "- Patient basic information\n",
    "- Laboratory test data\n",
    "- Dialysis adequacy data\n",
    "- Dialysis protocol data\n",
    "- Dialysis evaluation data\n",
    "- Complication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path \n",
    "base_info_path = 'data/PatientBasicInformation-NoName.xlsx'\n",
    "labtest_path = 'data/LaboratoryTestData-NoName.xlsx'\n",
    "dialysis_adequacy_path = 'data/DialysisAdequacyData-NoName.xlsx'\n",
    "dialysis_protocol_path = 'data/DialysisProtocolData-NoName.xlsx'\n",
    "dialysis_evaluation_path = 'data/DialysisEvaluationData-NoName.xlsx'\n",
    "complication_path = 'data/ComplicationData-NoName.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Information Table\n",
    "- Select feature subset: PDID, Date of Birth, Catheter Implantation Time, Gender, Diabetes, Primary Disease\n",
    "- Ensure PDID, Date of Birth, Catheter Implantation Time, Gender, Diabetes have no missing values\n",
    "- Get static features: Gender, Diabetes, Primary Disease (Diabetic Nephropathy, Glomerulonephritis, Hypertensive Renal Damage, Chronic Interstitial Nephritis, Autosomal Dominant Polycystic Kidney Disease, IGA)\n",
    "- Get date of birth, implantation time as baseline time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = pd.read_excel(base_info_path)\n",
    "print(df_base_info.columns.tolist())\n",
    "print(f'Number of patients {len(df_base_info)}')\n",
    "print(f'Number of features {len(df_base_info.columns.tolist())}')\n",
    "\n",
    "# step1. Select feature subset\n",
    "base_info_feature = ['PDID', 'Date of Birth', 'Catheter Implantation Time', 'Gender', 'Diabetes', 'Primary Disease']\n",
    "df_base_info = df_base_info[base_info_feature]\n",
    "\n",
    "# step2. Ensure PDID, Date of Birth, Catheter Implantation Time, Gender, Diabetes have no missing values\n",
    "df_base_info.dropna(subset=['PDID', 'Date of Birth', 'Catheter Implantation Time', 'Gender', 'Diabetes'], inplace=True)\n",
    "print(f'Number of patients {len(df_base_info)}')\n",
    "\n",
    "# step 3. Get static features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_static_feature = df_base_info[['PDID', 'Gender', 'Diabetes', 'Primary Disease']]\n",
    "df_static_feature['Primary Disease'] = df_static_feature['Primary Disease'].fillna('Missing')\n",
    "df_static_feature['Gender'] = le.fit_transform(df_static_feature['Gender'])\n",
    "df_static_feature['Diabetes'] = le.fit_transform(df_static_feature['Diabetes'])\n",
    "others_list = []\n",
    "static_data = {}\n",
    "for i in range(len(df_static_feature)):\n",
    "    tmp_data = df_static_feature.iloc[i]\n",
    "    static_data[tmp_data['PDID']] = [tmp_data['Gender'], tmp_data['Diabetes']]\n",
    "    key_list = ['Glomerulonephritis', 'Hypertension', 'Chronic Interstitial Nephritis', 'Polycystic', 'IGA']\n",
    "    flag = 0\n",
    "    if 'Diabetes' in tmp_data['Primary Disease'] or 'DN' in tmp_data['Primary Disease']:\n",
    "        static_data[tmp_data['PDID']].append(1)\n",
    "        flag = 1\n",
    "    else:\n",
    "        static_data[tmp_data['PDID']].append(0)\n",
    "    for key_word in key_list:\n",
    "        if key_word in tmp_data['Primary Disease']:\n",
    "            static_data[tmp_data['PDID']].append(1)\n",
    "            flag = 1\n",
    "        else:\n",
    "            static_data[tmp_data['PDID']].append(0)\n",
    "    if flag == 0:\n",
    "        others_list.append(tmp_data['Primary Disease'])\n",
    "\n",
    "df_base_info.to_csv('out/checkpoints/df_base_info.csv', encoding='utf_8_sig', index=False)\n",
    "pickle.dump(static_data, open('out/checkpoints/static_data.pkl', 'wb'))\n",
    "\n",
    "print(f'Proportion of primary diseases not covered: {len(others_list) / len(df_static_feature):.3f}')\n",
    "\n",
    "# step 4. Get date of birth, implantation time as baseline time\n",
    "df_basetime = df_base_info[['PDID', 'Date of Birth', 'Catheter Implantation Time']]\n",
    "\n",
    "basetime1 = {}\n",
    "basetime2 = {}\n",
    "for i in range(len(df_basetime)):\n",
    "    tmp = df_basetime.iloc[i]\n",
    "    basetime1[tmp['PDID']] = tmp['Date of Birth']\n",
    "    basetime2[tmp['PDID']] = tmp['Catheter Implantation Time']\n",
    "df_base_info.to_csv('out/checkpoints/df_base_info.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info.to_csv('out/checkpoints/df_base_info.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Labtest\n",
    "- Ensure PDID, Date have no missing values\n",
    "- Remove records with PDID not in the basic information table\n",
    "- Merge records of the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labtest = pd.read_excel(labtest_path)\n",
    "print(df_labtest.columns.tolist())\n",
    "print(f'Number of labtests: {len(df_labtest)}, Number of PDID: {len(set(df_labtest.PDID))}, Number of features: {len(df_labtest.columns)}')\n",
    "\n",
    "# step1. Ensure PDID, Date have no missing values\n",
    "df_labtest.dropna(subset=['PDID', 'Date'], inplace=True)\n",
    "print(f'after step 1: Number of labtests: {len(df_labtest)}, Number of PDID: {len(set(df_labtest.PDID))}, Number of features: {len(df_labtest.columns)}')\n",
    "\n",
    "# step2. Remove records with PDID not in the basic information table\n",
    "df_labtest = df_labtest.loc[[PDID in df_base_info['PDID'].tolist() for PDID in df_labtest['PDID']]]\n",
    "df_labtest = df_labtest[[column for column in df_labtest.columns if column != 'ID']]\n",
    "print(f'after step 2: Number of labtests: {len(df_labtest)}, Number of PDID: {len(set(df_labtest.PDID))}, Number of features: {len(df_labtest.columns)}')\n",
    "\n",
    "# step3. Merge records of the same day\n",
    "df_labtest = df_labtest.groupby(['PDID', 'Date'], as_index=False, dropna=True).mean()\n",
    "print(f'after step 3: Number of labtests: {len(df_labtest)}, Number of PDID: {len(set(df_labtest.PDID))}, Number of features: {len(df_labtest.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labtest.to_csv('out/checkpoints/df_labtest.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dialysis Adequacy\n",
    "- Ensure PDID, Date have no missing values\n",
    "- Remove records with PDID not in the basic information table\n",
    "- Merge records of the same day\n",
    "- Calculate relevant indicators based on dialysis adequacy formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dialysis_adequacy = pd.read_excel(dialysis_adequacy_path)\n",
    "print(df_dialysis_adequacy.columns.tolist())\n",
    "print(f'Number of dialysis_adequacy: {len(df_dialysis_adequacy)}, Number of PDID: {len(set(df_dialysis_adequacy.PDID))}, Number of features: {len(df_dialysis_adequacy.columns)}')\n",
    "\n",
    "# step1. Ensure PDID, Date have no missing values\n",
    "df_dialysis_adequacy.dropna(subset=['PDID', 'Date'], inplace=True)\n",
    "print(f'after step 1: Number of dialysis_adequacy: {len(df_dialysis_adequacy)}, Number of PDID: {len(set(df_dialysis_adequacy.PDID))}, Number of features: {len(df_dialysis_adequacy.columns)}')\n",
    "\n",
    "# step2. Remove records with PDID not in the basic information table\n",
    "df_dialysis_adequacy = df_dialysis_adequacy.loc[[PDID in df_base_info['PDID'].tolist() for PDID in df_dialysis_adequacy['PDID']]]\n",
    "print(f'after step 2: Number of dialysis_adequacy: {len(df_dialysis_adequacy)}, Number of PDID: {len(set(df_dialysis_adequacy.PDID))}, Number of features: {len(df_dialysis_adequacy.columns)}')\n",
    "\n",
    "# step3. Merge records of the same day\n",
    "df_dialysis_adequacy = df_dialysis_adequacy.groupby(['PDID', 'Date'], as_index=False, dropna=True).mean()\n",
    "print(f'after step 3: Number of dialysis_adequacy: {len(df_dialysis_adequacy)}, Number of PDID: {len(set(df_dialysis_adequacy.PDID))}, Number of features: {len(df_dialysis_adequacy.columns)}')\n",
    "\n",
    "# step4. Calculate relevant indicators based on dialysis adequacy formula\n",
    "sex = {}\n",
    "for i in range(len(df_base_info)):\n",
    "    tmp = df_base_info.iloc[i]\n",
    "    if tmp['Gender'] == 'Male':\n",
    "        sex[tmp['PDID']] = 1\n",
    "    elif tmp['Gender'] == 'Female':\n",
    "        sex[tmp['PDID']] = 2\n",
    "    else:\n",
    "        print(\"missing\")\n",
    "age_list = []\n",
    "for i in range(len(df_dialysis_adequacy)):\n",
    "    tmp = df_dialysis_adequacy.iloc[i]\n",
    "    age_list.append((tmp['Date'] - basetime1[tmp['PDID']]).days / 365)\n",
    "df_dialysis_adequacy['Age'] = age_list\n",
    "fc = lambda x: sex[x]\n",
    "sex_list = [fc(x) for x in df_dialysis_adequacy.PDID.tolist()]\n",
    "df_dialysis_adequacy['Gender'] = sex_list\n",
    "\n",
    "formula_use_feature = ['Age', 'Gender', 'Height', 'Actual Weight', 'Blood Urea', 'Blood Creatinine', '24-hour Urine Volume', '24-hour Urine Urea', '24-hour Urine Creatinine', '24-hour Dialysate Volume', '24-hour Dialysate Urea', '24-hour Dialysate Creatinine']\n",
    "df_calculate_adequacy = df_dialysis_adequacy[formula_use_feature]\n",
    "# Set urine urea and urine creatinine to 0 for records with urine volume of 0\n",
    "df_calculate_adequacy['24-hour Urine Urea'] = df_calculate_adequacy['24-hour Urine Urea'].fillna(0)\n",
    "df_calculate_adequacy['24-hour Urine Creatinine'] = df_calculate_adequacy['24-hour Urine Creatinine'].fillna(0)\n",
    "\n",
    "weight = [[], []]\n",
    "BSA = [[], []]\n",
    "V = [[], []]\n",
    "GFR = [[], []]\n",
    "Krt = [[], []]\n",
    "Kpt = [[], []]\n",
    "Kt = [[], []]\n",
    "CrCr = [[], []]\n",
    "CpCr = [[], []]\n",
    "CCr = [[], []]\n",
    "nPNA = [[], []]\n",
    "for i in range(len(df_calculate_adequacy)):\n",
    "    tmp = df_calculate_adequacy.iloc[i]\n",
    "    if tmp['Gender'] == 1:\n",
    "        weight[0].append(tmp['Height'] - 105)\n",
    "    elif tmp['Gender'] == 2:\n",
    "        weight[0].append(tmp['Height'] - 110)\n",
    "    weight[1].append(tmp['Actual Weight'])\n",
    "    BSA[0].append(0.007184 * tmp['Height']**0.725 * tmp['Actual Weight']**0.425)\n",
    "    BSA[1].append(0.007184 * tmp['Height']**0.725 * tmp['Actual Weight']**0.425)\n",
    "    if tmp['Gender'] == 1:\n",
    "        V[0].append(2.477 + (0.3362 * weight[0][-1]) + (0.1074 * tmp['Height']) - (0.09516 * tmp['Age']))\n",
    "        V[1].append(2.477 + (0.3362 * tmp['Actual Weight']) + (0.1074 * tmp['Height']) - (0.09516 * tmp['Age']))\n",
    "    elif tmp['Gender'] == 2:\n",
    "        V[0].append(-2.097 + (0.2466 * weight[0][-1]) + (0.1069 * tmp['Height']))\n",
    "        V[1].append(-2.097 + (0.2466 * tmp['Actual Weight']) + (0.1069 * tmp['Height']))\n",
    "    GFR[0].append((tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] + tmp['24-hour Urine Creatinine'] * tmp['24-hour Urine Volume'] * 1.73 / tmp['Blood Creatinine'] / BSA[0][-1]) / 2 / 1440)\n",
    "    GFR[1].append((tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] + tmp['24-hour Urine Creatinine'] * tmp['24-hour Urine Volume'] * 1.73 / tmp['Blood Creatinine'] / BSA[1][-1]) / 2 / 1440)\n",
    "    Krt[0].append((tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] / V[0][-1] / 1000) * 7)\n",
    "    Krt[1].append((tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] / V[1][-1] / 1000) * 7)\n",
    "    Kpt[0].append((tmp['24-hour Dialysate Urea'] * tmp['24-hour Dialysate Volume'] / tmp['Blood Urea'] / V[0][-1] / 1000) * 7)\n",
    "    Kpt[1].append((tmp['24-hour Dialysate Urea'] * tmp['24-hour Dialysate Volume'] / tmp['Blood Urea'] / V[1][-1] / 1000) * 7)\n",
    "    Kt[0].append(Krt[0][-1] + Kpt[0][-1])\n",
    "    Kt[1].append(Krt[1][-1] + Kpt[1][-1])\n",
    "    CrCr[0].append(((tmp['24-hour Urine Creatinine'] * tmp['24-hour Urine Volume'] / tmp['Blood Creatinine'] / 1000 + tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] / 1000) / 2) * 1.73 / BSA[0][-1] * 7)\n",
    "    CrCr[1].append(((tmp['24-hour Urine Creatinine'] * tmp['24-hour Urine Volume'] / tmp['Blood Creatinine'] / 1000 + tmp['24-hour Urine Urea'] * tmp['24-hour Urine Volume'] / tmp['Blood Urea'] / 1000) / 2) * 1.73 / BSA[1][-1] * 7)\n",
    "    CpCr[0].append((tmp['24-hour Dialysate Creatinine'] * tmp['24-hour Dialysate Volume'] / tmp['Blood Creatinine'] / 1000) * 1.73 / BSA[0][-1] * 7)\n",
    "    CpCr[1].append((tmp['24-hour Dialysate Creatinine'] * tmp['24-hour Dialysate Volume'] / tmp['Blood Creatinine'] / 1000) * 1.73 / BSA[1][-1] * 7)\n",
    "    CCr[0].append(CrCr[0][-1] + CpCr[0][-1])\n",
    "    CCr[1].append(CrCr[0][-1] + CpCr[1][-1])\n",
    "\n",
    "df_adequacy = df_dialysis_adequacy[['PDID', 'Date']]\n",
    "df_adequacy['weight_0'] = weight[0]\n",
    "df_adequacy['BSA_0'] = BSA[0]\n",
    "df_adequacy['V_0'] = V[0]\n",
    "df_adequacy['GFR_0'] = GFR[0]\n",
    "df_adequacy['Krt_0'] = Krt[0]\n",
    "df_adequacy['Kpt_0'] = Kpt[0]\n",
    "df_adequacy['Kt_0'] = Kt[0]\n",
    "df_adequacy['CrCr_0'] = CrCr[0]\n",
    "df_adequacy['CpCr_0'] = CpCr[0]\n",
    "df_adequacy['CCr_0'] = CCr[0]\n",
    "df_adequacy['weight_1'] = weight[1]\n",
    "df_adequacy['BSA_1'] = BSA[1]\n",
    "df_adequacy['V_1'] = V[1]\n",
    "df_adequacy['GFR_1'] = GFR[1]\n",
    "df_adequacy['Krt_1'] = Krt[1]\n",
    "df_adequacy['Kpt_1'] = Kpt[1]\n",
    "df_adequacy['Kt_1'] = Kt[1]\n",
    "df_adequacy['CrCr_1'] = CrCr[1]\n",
    "df_adequacy['CpCr_1'] = CpCr[1]\n",
    "df_adequacy['CCr_1'] = CCr[1]\n",
    "\n",
    "print(f'after step 4: Number of dialysis_adequacy: {len(df_adequacy)}, Number of PDID: {len(set(df_adequacy.PDID))}, Number of features: {len(df_dialysis_adequacy.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adequacy.to_csv('out/checkpoints/df_adequacy.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dialysis Protocol\n",
    "- Ensure PDID, Date have no missing values\n",
    "- Remove records with PDID not in the basic information table\n",
    "- Merge records of the same day dialysis, calculate dialysis dose and glucose exposure (multiply horizontally, add vertically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dialysis_protocol = pd.read_excel(dialysis_protocol_path)\n",
    "print(df_dialysis_protocol.columns.tolist())\n",
    "print(f'Number of dialysis_protocol: {len(df_dialysis_protocol)}, Number of PDID: {len(set(df_dialysis_protocol.PDID))}, Number of features: {len(df_dialysis_protocol.columns)}')\n",
    "\n",
    "# step1. Ensure PDID, Date have no missing values\n",
    "df_dialysis_protocol.dropna(subset=['PDID', 'Date'], inplace=True)\n",
    "print(f'after step 1: Number of dialysis_protocol: {len(df_dialysis_protocol)}, Number of PDID: {len(set(df_dialysis_protocol.PDID))}, Number of features: {len(df_dialysis_protocol.columns)}')\n",
    "\n",
    "# step2. Remove records with PDID not in the basic information table\n",
    "df_dialysis_protocol = df_dialysis_protocol.loc[[PDID in df_base_info['PDID'].tolist() for PDID in df_dialysis_protocol['PDID']]]\n",
    "print(f'after step 2: Number of dialysis_protocol: {len(df_dialysis_protocol)}, Number of PDID: {len(set(df_dialysis_protocol.PDID))}, Number of features: {len(df_dialysis_protocol.columns)}')\n",
    "\n",
    "# step3. Merge records of the same day dialysis, calculate dialysis dose and glucose exposure (multiply horizontally, add vertically)\n",
    "PDID_list = []\n",
    "date_list = []\n",
    "dosage_list = []  # Dialysate dose\n",
    "for _, tmp in df_dialysis_protocol.groupby(['PDID', 'Date']):\n",
    "    x1 = 0\n",
    "    for j in range(len(tmp)):\n",
    "        x1 += tmp.iloc[j]['Dialysate Dose'] * tmp.iloc[j]['Dialysis Frequency']\n",
    "    PDID_list.append(tmp.iloc[-1]['PDID'])\n",
    "    date_list.append(tmp.iloc[-1]['Date'])\n",
    "    dosage_list.append(x1)\n",
    "\n",
    "df_protocol = pd.DataFrame({'PDID': PDID_list, 'Date': date_list, 'Dialysate Dose': dosage_list})\n",
    "print(f'after step 3: Number of dialysis_protocol: {len(df_protocol)}, Number of PDID: {len(set(df_protocol.PDID))}, Number of features: {len(df_protocol.columns)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocol.to_csv('out/checkpoints/df_protocol.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dialysis Evaluation\n",
    "- Ensure PDID, Date have no missing values\n",
    "- Remove records with PDID not in the basic information table\n",
    "- Keep part of the features\n",
    "- Merge records of the same day dialysis\n",
    "df_dialysis_evalua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dialysis_evaluation = pd.read_excel(dialysis_evaluation_path)\n",
    "print(df_dialysis_evaluation.columns.tolist())\n",
    "print(f'Number of dialysis_evaluation: {len(df_dialysis_evaluation)}, Number of PDID: {len(set(df_dialysis_evaluation.PDID))}, Number of features: {len(df_dialysis_evaluation.columns)}')\n",
    "\n",
    "# step1. Ensure PDID, Date have no missing values\n",
    "df_dialysis_evaluation.dropna(subset=['PDID', 'Date'], inplace=True)\n",
    "print(f'after step 1: Number of dialysis_evaluation: {len(df_dialysis_evaluation)}, Number of PDID: {len(set(df_dialysis_evaluation.PDID))}, Number of features: {len(df_dialysis_evaluation.columns)}')\n",
    "\n",
    "# step2. Remove records with PDID not in the basic information table\n",
    "df_dialysis_evaluation = df_dialysis_evaluation.loc[[PDID in df_base_info['PDID'].tolist() for PDID in df_dialysis_evaluation['PDID']]]\n",
    "print(f'after step 2: Number of dialysis_evaluation: {len(df_dialysis_evaluation)}, Number of PDID: {len(set(df_dialysis_evaluation.PDID))}, Number of features: {len(df_dialysis_evaluation.columns)}')\n",
    "\n",
    "# step3. Keep part of the features\n",
    "evaluation_use_feature = ['PDID', 'Date', 'Home Systolic Blood Pressure', 'Home Diastolic Blood Pressure', 'Heart Rate', 'Actual Weight', 'Edema', 'Urine Volume', 'Ultrafiltration Volume']\n",
    "df_dialysis_evaluation = df_dialysis_evaluation[evaluation_use_feature]\n",
    "print(f'after step 3: Number of dialysis_evaluation: {len(df_dialysis_evaluation)}, Number of PDID: {len(set(df_dialysis_evaluation.PDID))}, Number of features: {len(df_dialysis_evaluation.columns)}')\n",
    "\n",
    "# step4. Merge records of the same day dialysis\n",
    "df_evaluation = df_dialysis_evaluation.groupby(['PDID', 'Date'], as_index=False, dropna=True).mean()\n",
    "print(f'after step 4: Number of dialysis_evaluation: {len(df_dialysis_evaluation)}, Number of PDID: {len(set(df_dialysis_evaluation.PDID))}, Number of features: {len(df_dialysis_evaluation.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation.to_csv('out/checkpoints/df_evaluation.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Complications\n",
    "- Ensure PDID, Date have no missing values\n",
    "- Remove records with PDID not in the basic information table\n",
    "- Keep part of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complication = pd.read_excel(complication_path)\n",
    "print(df_complication.columns.tolist())\n",
    "print(f'Number of complications: {len(df_complication)}, Number of PDID: {len(set(df_complication.PDID))}, Number of features: {len(df_complication.columns)}')\n",
    "\n",
    "# step1. Ensure PDID, Date have no missing values\n",
    "df_complication.dropna(subset=['PDID', 'Date of Onset'], inplace=True)\n",
    "print(f'after step 1: Number of complications: {len(df_complication)}, Number of PDID: {len(set(df_complication.PDID))}, Number of features: {len(df_complication.columns)}')\n",
    "\n",
    "# step2. Remove records with PDID not in the basic information table\n",
    "df_complication = df_complication.loc[[PDID in df_base_info['PDID'].tolist() for PDID in df_complication['PDID']]]\n",
    "print(f'after step 2: Number of complications: {len(df_complication)}, Number of PDID: {len(set(df_complication.PDID))}, Number of features: {len(df_complication.columns)}')\n",
    "\n",
    "# step3. Keep part of the features\n",
    "complication_date = df_complication['Date of Onset']\n",
    "df_tmp = df_complication[['PDID', 'Disease Category', 'Disease Name']]\n",
    "df_tmp['Date'] = complication_date\n",
    "category_list = df_tmp['Disease Category'].tolist()\n",
    "category1_list = [1 if item == 'Respiratory System' else 0 for item in category_list]\n",
    "category2_list = [1 if item == 'Peritoneal Dialysis-Related Complications' else 0 for item in category_list]\n",
    "category3_list = [1 if item == 'Cardiovascular System' else 0 for item in category_list]\n",
    "category4_list = [1 if item == 'Digestive System' else 0 for item in category_list]\n",
    "name_list = df_tmp['Disease Name'].tolist()\n",
    "name1_list = [1 if item == 'Acute Upper Respiratory Tract Infection' else 0 for item in name_list]\n",
    "name2_list = [1 if item == 'Acute Upper Respiratory Tract Infection' else 0 for item in name_list]\n",
    "df_tmp = df_tmp[['PDID', 'Date']]\n",
    "df_tmp['Respiratory System'] = category1_list\n",
    "df_tmp['Peritoneal Dialysis-Related Complications'] = category2_list\n",
    "df_tmp['Cardiovascular System'] = category3_list\n",
    "df_tmp['Digestive System'] = category4_list\n",
    "df_tmp['Acute Upper Respiratory Tract Infection'] = name1_list\n",
    "df_tmp['Peritoneal Dialysis-Related Peritonitis'] = name2_list\n",
    "\n",
    "# There may be multiple records in one day, complications and other information are handled separately\n",
    "df_tmp1 = df_tmp[['PDID', 'Date']].drop_duplicates(subset=['PDID', 'Date'])\n",
    "df_tmp2 = df_tmp[['PDID', 'Date', 'Respiratory System', 'Peritoneal Dialysis-Related Complications', 'Cardiovascular System', 'Digestive System', 'Acute Upper Respiratory Tract Infection', 'Peritoneal Dialysis-Related Peritonitis']].groupby(['PDID', 'Date'], as_index=False, dropna=False).sum()\n",
    "df_tmp2 = df_tmp2[['Respiratory System', 'Peritoneal Dialysis-Related Complications', 'Cardiovascular System', 'Digestive System', 'Acute Upper Respiratory Tract Infection', 'Peritoneal Dialysis-Related Peritonitis']]\n",
    "df_tmp1.reset_index(drop=True, inplace=True)\n",
    "df_tmp2.reset_index(drop=True, inplace=True)\n",
    "df_tmp = pd.concat([df_tmp1, df_tmp2], axis=1)\n",
    "df_complication = df_tmp\n",
    "\n",
    "print(f'after step 3: Number of complications: {len(df_complication)}, Number of PDID: {len(set(df_complication.PDID))}, Number of features: {len(df_complication.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complication.to_csv('out/checkpoints/df_complication.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Merge labtest-related tables, dialysis-related tables, and complication table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. Merge each table\n",
    "df_tmp = pd.merge(df_labtest, df_adequacy, how='outer', on=['PDID', 'Date'])\n",
    "df_tmp = pd.merge(df_tmp, df_protocol, how='outer', on=['PDID', 'Date'])\n",
    "df_tmp = pd.merge(df_tmp, df_evaluation, how='outer', on=['PDID', 'Date'])\n",
    "df_tmp = pd.merge(df_tmp, df_complication, how='outer', on=['PDID', 'Date'])\n",
    "df_tmp.sort_values(by=['PDID', 'Date'], inplace=True)\n",
    "df_all = df_tmp\n",
    "print(f'after step 1: Number of all: {len(df_all)}, Number of PDID: {len(set(df_all.PDID))}, Number of features: {len(df_all.columns)}')\n",
    "\n",
    "# step 2. Remove rows without records\n",
    "df_all.dropna(how='all', subset=[feature for feature in df_all.columns if feature not in ['PDID', 'Date']], inplace=True)\n",
    "print(f'after step 2: Number of all: {len(df_all)}, Number of PDID: {len(set(df_all.PDID))}, Number of features: {len(df_all.columns)}')\n",
    "\n",
    "# step 3. Add 2 features\n",
    "to_basetime1 = []\n",
    "to_basetime2 = []\n",
    "for i in range(len(df_tmp)):\n",
    "    tmp = df_tmp.iloc[i]\n",
    "    to_basetime1.append((tmp['Date'] - basetime1[tmp['PDID']]).days)\n",
    "    to_basetime2.append((tmp['Date'] - basetime2[tmp['PDID']]).days)\n",
    "\n",
    "df_all['to_basetime1'] = to_basetime1\n",
    "df_all['to_basetime2'] = to_basetime2\n",
    "df_all['to_basetime1'] = df_all['to_basetime1'] / 365\n",
    "df_all['to_basetime2'] = df_all['to_basetime2'] / 365\n",
    "df_all.rename(columns={'to_basetime1': 'Age'}, inplace=True)\n",
    "df_all.rename(columns={'to_basetime2': 'Dialysis Time'}, inplace=True)\n",
    "\n",
    "print(f'after step 3: Number of all: {len(df_all)}, Number of PDID: {len(set(df_all.PDID))}, Number of features: {len(df_all.columns)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get patient outcomes\n",
    "- Complete outcome information\n",
    "- Select required patient information\n",
    "- Use the last time point in the dataset to complete the outcome time (patients who did not exit peritoneal dialysis are assumed to be still receiving treatment at the end of the window)\n",
    "- Outcome result: death as one category, others as another, record the outcome time of each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 Complete outcome information\n",
    "df_reversion = pd.read_excel(base_info_path)[['PDID', 'Exit Peritoneal Dialysis Time', 'Reason for Exit', 'Outcome']]\n",
    "na_reason = df_reversion['Reason for Exit'].isnull()\n",
    "na_outcome = df_reversion['Outcome'].isnull()\n",
    "indices = (~na_reason & na_outcome)  # Has exit reason but no outcome\n",
    "df_reversion.loc[indices, 'Outcome'] = df_reversion.loc[indices, 'Reason for Exit']\n",
    "\n",
    "# step2 Select required patient information\n",
    "indices = df_reversion['PDID'].isin(df_all['PDID'])\n",
    "df_reversion = df_reversion[indices]\n",
    "\n",
    "na_exit_time = df_reversion['Exit Peritoneal Dialysis Time'].isnull()\n",
    "na_outcome = df_reversion['Outcome'].isnull()\n",
    "print(f'All patients who exited peritoneal dialysis have (1) exit time (2) exit reason: {(na_exit_time != na_outcome).sum() == 0}')\n",
    "\n",
    "# step3 Use the last time point in the dataset to complete the outcome time (patients who did not exit peritoneal dialysis are assumed to be still receiving treatment at the end of the window)\n",
    "df_reversion['Exit Peritoneal Dialysis Time'] = df_reversion['Exit Peritoneal Dialysis Time'].fillna(df_all['Date'].max())\n",
    "df_reversion['Outcome'] = df_reversion['Outcome'].fillna('Still receiving treatment')\n",
    "\n",
    "# step4 Outcome result: death as one category, others as another, record the outcome time of each patient\n",
    "death_filter = [(\"death\" in item or 'Multiple Organ Failure' in item or \"General Failure\" in item or \"Systemic Organ Failure\" in item or \"Shock\" in item) and \"Lost Contact\" not in item for item in df_reversion.Outcome]\n",
    "df_death = df_reversion[death_filter]\n",
    "df_others = df_reversion[~np.array(death_filter)]\n",
    "reversion = {}\n",
    "reversion_time = {}\n",
    "for i in range(len(df_death)):\n",
    "    tmp = df_death.iloc[i]\n",
    "    reversion[tmp['PDID']] = 1\n",
    "    reversion_time[tmp['PDID']] = tmp['Exit Peritoneal Dialysis Time']\n",
    "for i in range(len(df_others)):\n",
    "    tmp = df_others.iloc[i]\n",
    "    reversion[tmp['PDID']] = 0\n",
    "    reversion_time[tmp['PDID']] = tmp['Exit Peritoneal Dialysis Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(reversion, open('out/checkpoints/reversion.pkl','wb'))\n",
    "pickle.dump(reversion_time, open('out/checkpoints/reversion_time.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handle conflicting data\n",
    "- Patient still has labtest/dialysis/complication records after the death time point: delete all records of the patient, corresponding PDID is saved as contradictory_PDID_1.csv\n",
    "- Patient is alive, but still has labtest/dialysis/complication records after the recorded outcome time point (no longer on peritoneal dialysis, using other treatment options still being checked): delete records after the outcome time point, corresponding PDID is saved as contradictory_PDID_2.csv\n",
    "- Delete records before 2011-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 Handle records after the outcome time point\n",
    "contradictory_PDID_1_list = []\n",
    "contradictory_PDID_2_list = []\n",
    "df_all_new = pd.DataFrame()\n",
    "for PDID, tmp_df in df_all.groupby('PDID'):\n",
    "    tmp_df.sort_values(by='Date', inplace=True)\n",
    "    if tmp_df.iloc[-1]['Date'] > reversion_time[PDID]:\n",
    "        if reversion[PDID] == 1:\n",
    "            contradictory_PDID_1_list.append(PDID)\n",
    "            contradictory_PDID_2_list.append(PDID)\n",
    "            df_all_new = pd.concat([df_all_new, tmp_df[tmp_df['Date'] < reversion_time[PDID]]], axis=0)\n",
    "    else:\n",
    "        df_all_new = pd.concat([df_all_new, tmp_df], axis=0)\n",
    "pd.DataFrame({'PDID': contradictory_PDID_1_list}).to_csv('out/checkpoints/contradictory_PDID_1.csv', index=False)\n",
    "pd.DataFrame({'PDID': contradictory_PDID_2_list}).to_csv('out/checkpoints/contradictory_PDID_2.csv', index=False)\n",
    "df_all = df_all_new\n",
    "\n",
    "# step2 Delete records before 2011-01-01\n",
    "start_time = pd.to_datetime(pd.Series('2011-01-01'))[0]\n",
    "df_all = df_all.loc[[date > start_time for date in df_all['Date']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('out/checkpoints/df_all.csv',encoding='utf_8_sig',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
